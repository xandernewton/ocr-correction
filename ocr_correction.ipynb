{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67e7fec-653c-4461-9c03-3ced768e6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, set_seed, T5ForConditionalGeneration, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field\n",
    "import nlpaug.augmenter.char as nac\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a72f1326-c2e1-49ea-9023-4178fee0b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments():\n",
    "    model_name_or_path: str\n",
    "    tokenizer_name: str\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments():\n",
    "    max_len: int\n",
    "   \n",
    "@dataclass\n",
    "class TrainingArguments_2():\n",
    "    output_dir: str\n",
    "    overwrite_output_dir: bool\n",
    "    per_device_train_batch_size:int\n",
    "    per_device_eval_batch_size:int\n",
    "    gradient_accumulation_steps:int\n",
    "    learning_rate: float\n",
    "    warmup_steps: int\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    num_train_epochs: int\n",
    "    do_train: bool\n",
    "    do_eval: bool\n",
    "    fp16: bool\n",
    "    max_steps: int\n",
    "    seed: int\n",
    "    report_to: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a0028b7-33cd-4b1c-948c-c678687c6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"output_dir\": './byt5-base-english-ocr-correction',\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\":4,\n",
    "    \"logging_steps\": 1000,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"warmup_steps\": 250,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"dataloader_pin_memory\": True,\n",
    "    \"eval_steps\": 1000,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"seed\":123,\n",
    "    \"report_to\":\"wandb\",\n",
    "    \"fp16\":False,\n",
    "    \"run_name\":\"byt5-base-english-ocr-correction-2\",\n",
    "    \"weight_decay\":0.01,\n",
    "    \"evaluation_strategy\":\"steps\",\n",
    "    \"disable_tqdm\":False,\n",
    "    \"auto_find_batch_size\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22db6f75-a988-4e6a-9d88-db4e33fe4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments_2))\n",
    "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aea349e-72c5-4d2c-bdf1-9e5532cd61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(batch, tokenizer, length):\n",
    "\n",
    "    chunks = []\n",
    "    for text in batch: \n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = [word for word in text if len(word) > 1]\n",
    "        chunks.extend([' '.join(text[i:i + length]) for i in range(0, len(text), length)])\n",
    "    \n",
    "    return {'text': chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e84d507-d940-4546-ac85-da9ca9994f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/studio-lab-user/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 545.23it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 227.27ba/s]\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-73dab13e0d557913.arrow\n",
      "100%|██████████| 4/4 [00:00<00:00, 221.40ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 67.72ba/s]\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-64a5410963e22a82.arrow\n",
      "100%|██████████| 3/3 [00:00<00:00, 126.48ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.99ba/s]\n",
      "100%|██████████| 890/890 [00:37<00:00, 23.46ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.92ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n",
      "100%|██████████| 1143/1143 [08:55<00:00,  2.13ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    }
   ],
   "source": [
    "#dataset = load_dataset('csv', data_files={'train': ['data/nl_unshuffled_train_100_000.csv'], 'test': 'data/nl_unshuffled_test_10_000.csv'})\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "dataset = dataset.filter(lambda x: x[\"text\"] != '')\n",
    "dataset = dataset.filter(lambda x: not x['text'].startswith(\" = = \"))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset = dataset.map(\n",
    "    lambda x: chunk_text(x['text'],\n",
    "                         tokenizer,\n",
    "                         128),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "def ocr_augment_chars(text, **kwargs):\n",
    "    aug = nac.OcrAug(**kwargs)\n",
    "\n",
    "    augmented_data = aug.augment(text)\n",
    "    return augmented_data\n",
    "\n",
    "# Augmenting the dataset with common OCR errors\n",
    "dataset = dataset.map(lambda x: {'ocr_text' : ocr_augment_chars(x['text'], aug_char_p =0.4, aug_word_p = 0.6), 'text': x['text']}, batched=True,  remove_columns=[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e719b0c-0e97-46e3-8b76-83418f51a553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The game began development in 2010 carrying over large portion of the work done on Valkyria Chronicles II While it retained the standard features of the series it also underwent multiple adjustments such as making the game more forgiving for series newcomers Character designer unk Honjou and composer Hitoshi Sakimoto both returned from previous entries along with Valkyria Chronicles II director Takeshi Ozawa large team of writers handled the script The game opening theme was sung by May',\n",
       " 'ocr_text': 'The game began development in 2010 carrying over large portion 0f the work done on Valkyria Chronicles II While it retained the standard featoke8 of the series it also underwent multiple adjustments such a8 making the game more forgiving for series newcomers Ghakactek de8i9nek unk Honjou and composer Hitoshi Sakimoto 6uth returned from previous entries along with Valkyria Chronicles II director Takeshi D2awa large team 0f writers handled the script The game opening theme was sung by May'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c0632f-0b86-48c2-8494-45bd05c4471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142774\n",
      "2838\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['test']\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45fe4485-4b35-43da-8d94-565d3c8233eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.53k/2.53k [00:00<00:00, 728kB/s]\n",
      "Downloading: 100%|██████████| 698/698 [00:00<00:00, 327kB/s]\n",
      "Downloading: 100%|██████████| 2.44k/2.44k [00:00<00:00, 873kB/s]\n",
      "Downloading: 100%|██████████| 1.12G/1.12G [00:13<00:00, 89.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    max_length=data_args.max_len\n",
    ")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_args.model_name_or_path\n",
    ")\n",
    "\n",
    "# overwriting the default max_length of 20 \n",
    "tokenizer.model_max_length=128\n",
    "model.config.max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c168e18-f5bb-44ab-bfa7-b221f7f65513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(batch, max_len):\n",
    "    input_encodings = tokenizer.batch_encode_plus(batch['ocr_text'],\n",
    "                                                  truncation=True,\n",
    "                                                  padding='max_length',\n",
    "                                                  max_length=max_len\n",
    "                                                  )\n",
    "    target_encodings = tokenizer.batch_encode_plus(batch['text'],\n",
    "                                                   truncation=True,\n",
    "                                                   padding='max_length',\n",
    "                                                   max_length=max_len)\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'target_ids': target_encodings['input_ids'],\n",
    "        'target_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "683bd627-bdad-4bbb-9c23-0280ba65db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(tokenizer, dataset, max_len):\n",
    "    dataset = dataset.map(convert_to_features, fn_kwargs={\"max_len\":max_len}, batched=True)\n",
    "    # Set the tensor type and the columns which the dataset should return\n",
    "    columns = ['input_ids', 'target_ids',\n",
    "               'attention_mask', 'target_attention_mask']\n",
    "    dataset.with_format(type='torch', columns=columns)\n",
    "    # Rename columns to the names that the forward method of the selected\n",
    "    # model expects\n",
    "    dataset = dataset.rename_column('target_ids', 'labels')\n",
    "    dataset = dataset.rename_column('target_attention_mask', 'decoder_attention_mask')\n",
    "    dataset = dataset.remove_columns(['text', 'ocr_text'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45a332e2-508e-451e-a506-607de4d5c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1143/1143 [16:15<00:00,  1.17ba/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.22ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prep_dataset(tokenizer, train_dataset, data_args.max_len)\n",
    "valid_dataset = prep_dataset(tokenizer, valid_dataset, data_args.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9453160-ce6d-494f-a0bc-f59910b0ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk(\".\")\n",
    "valid_dataset.save_to_disk(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d628f11f-4fd2-41ff-8cfb-32e74f649ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(**vars(training_args))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02768bb8-9934-4001-9cc9-bfb4a6094d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    model_path=model_args.model_name_or_path if os.path.isdir(\n",
    "    model_args.model_name_or_path) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c4de0-336e-4832-9592-1383e5b2fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "# For convenience, we also re-save the tokenizer to the same directory,\n",
    "# so that you can share your model easily on huggingface.co/models =)\n",
    "tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bf7ed2e-0364-49b7-b2f2-c2ebc5d987b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': './byt5-base-dutch-ocr-correction',\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 2,\n",
       " 'per_device_eval_batch_size': 2,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'learning_rate': 0.0005,\n",
       " 'warmup_steps': 250,\n",
       " 'logging_steps': 100,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'eval_steps': 250,\n",
       " 'num_train_epochs': 4,\n",
       " 'do_train': True,\n",
       " 'do_eval': True,\n",
       " 'fp16': False,\n",
       " 'use_cache': False,\n",
       " 'max_steps': 5000,\n",
       " 'seed': 123}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd9c704-ce7b-4ecd-aed2-4c94b268c35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0984442941848cfbbb816e7181cb2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd143af1-27ce-4d93-aa99-25a8cf18e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.82k/2.82k [00:00<00:00, 1.90MB/s]\n",
      "Downloading: 100%|██████████| 2.44k/2.44k [00:00<00:00, 1.72MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('yelpfeast/byt5-base-english-ocr-correction', use_auth_token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yelpfeast/byt5-base-english-ocr-correction\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e185aff-3e29-4692-9159-add11b6b566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.OcrAug(aug_char_p =0.4, aug_word_p = 0.6)\n",
    "corrected_text = \"Life is like a box of chocolates\"\n",
    "augmented_text = aug.augment(corrected_text)\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2587dc0b-74f7-4f8e-90ea-e3f7e486c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Life is like a box of chocolates']\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([list(augmented_text.encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(corrected_text.encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "inputs = tokenizer(augmented_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "output_sequences = model.generate(\n",
    "\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    "\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d953e044-bcc2-47ff-80ee-375632fbb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "aug = nac.OcrAug(aug_char_p =0.4, aug_word_p = 0.6)\n",
    "corrected_text = \"Life is like a box of chocolates\"\n",
    "augmented_text = aug.augment(corrected_text)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('yelpfeast/byt5-base-english-ocr-correction')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "612fd179-eb1d-476e-bb7a-4703550af95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life i8 like a 6ux uf chocolates\n",
      "['Life is like a box of chocolates']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "aug = nac.OcrAug(aug_char_p =0.4, aug_word_p = 0.6)\n",
    "corrected_text = \"Life is like a box of chocolates\"\n",
    "augmented_text = aug.augment(corrected_text)\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('yelpfeast/byt5-base-english-ocr-correction', use_auth_token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yelpfeast/byt5-base-english-ocr-correction\", use_auth_token=True)\n",
    "\n",
    "inputs = tokenizer(augmented_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    "\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c208e7-4487-451b-ad46-2ecb9590f980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr:Python",
   "language": "python",
   "name": "conda-env-ocr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
